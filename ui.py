# -*- coding: utf-8 -*-
"""UI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLATf5EmO-UuuUZy6_FxKaYyHC35QmfL
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# # Example Python code
# def greet(name):
#     return f"Hello, {name}!"
# 
# print(greet("World"))
#

import os
os.system("python app.py")

!ls

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

!pip install torch==2.3.1+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

!pip install sentence-transformers

!pip install autoawq

from transformers import AutoTokenizer, AutoModelForCausalLM

# Define the path where the model is saved
model_save_path = '/content/drive/MyDrive/TheBloke_Mistral-7B-v0.1-AWQ'

# Load the tokenizer and model using the slow tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_save_path, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_save_path).to('cuda')

print("Model and tokenizer loaded successfully!")

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import os

# Step 1: Paths to the saved files in Google Drive

# Path to dataset embeddings and document chunks
embeddings_path = '/content/drive/MyDrive/chapter_document_embeddings.npy'
dataset_folder_path = '/content/drive/MyDrive/saved_dataset'
model_name = 'all-MiniLM-L6-v2'  # Same model used for embeddings

# Path to document chunks and their embeddings
chunks_path = '/content/drive/MyDrive/document_chunks.npy'
chunk_embeddings_path = '/content/drive/MyDrive/chunk_embeddings.npy'

# Step 2: Load all components

# Load document embeddings
print("Loading document embeddings...")
document_embeddings = np.load(embeddings_path)

# Load the Sentence Transformer model (for embedding user queries)
print("Loading Sentence Transformer model...")
embed_model = SentenceTransformer(model_name)

# Load the CSV files (to retrieve the original document text)
print("Loading CSV files from dataset...")
csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]
documents = []
for csv_file in csv_files:
    file_path = os.path.join(dataset_folder_path, csv_file)
    df = pd.read_csv(file_path)
    documents.extend(df['Extracted_Text'].tolist())

# Load document chunks and chunk embeddings
print("Loading document chunks and embeddings...")
document_chunks = np.load(chunks_path, allow_pickle=True)
chunk_embeddings = np.load(chunk_embeddings_path)

print("All components loaded successfully!")

!pip install streamlit

!pip install pyngrok

from pyngrok import ngrok

# Replace 'your_auth_token' with the actual token from ngrok
ngrok.set_auth_token("2nSyUvOPFL6BhaLjuyUct7ulLEf_6FpCg7Kouxxn3miLTpU6A")

from google.colab import files
uploaded = files.upload()  # This will open a file picker in Colab

from google.colab import files
uploaded = files.upload()  # This will open a file picker in Colab

!ls

!pip install python-docx

!pip install pymupdf

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import re
# import os
# import time
# from io import BytesIO
# from docx import Document
# import fitz
# from PIL import Image
# from testing import retrieve_relevant_chunk, generate_response, embed_model, chunk_embeddings, document_chunks
# from sentence_transformers import util
# 
# # PDF Files for Display
# pdf_files = {
#     "Chapter 1": "/content/drive/MyDrive/SQL_PDFs/Chapter 1.pdf",
#     "Chapter 2": "/content/drive/MyDrive/SQL_PDFs/Chapter 2.pdf",
#     "Chapter 3": "/content/drive/MyDrive/SQL_PDFs/Chapter 3.pdf",
#     "Chapter 4": "/content/drive/MyDrive/SQL_PDFs/Chapter 4.pdf",
#     "Chapter 5": "/content/drive/MyDrive/SQL_PDFs/Chapter 5.pdf",
#     "Chapter 6": "/content/drive/MyDrive/SQL_PDFs/Chapter 6.pdf",
#     "Chapter 7": "/content/drive/MyDrive/SQL_PDFs/Chapter 7.pdf"
# }
# 
# # Function to create a download button for PDFs in the sidebar
# def generate_download_button_for_pdfs():
#     st.sidebar.header("Available PDFs")
#     st.sidebar.write("Click to download:")
# 
#     for pdf_name, pdf_path in pdf_files.items():
#         st.sidebar.write(f"{pdf_name}")
#         with open(pdf_path, "rb") as f:
#             pdf_data = f.read()
# 
#         st.sidebar.download_button(
#             label=f"Download {pdf_name}",
#             data=pdf_data,
#             file_name=os.path.basename(pdf_path),
#             mime="application/pdf"
#         )
# 
# # Function to create a download link for Word file
# def generate_download_link_as_word(questions, responses, feedback_summary, filename, label):
#     doc = Document()
#     doc.add_heading('Feedback on Responses', 0)
# 
#     # Adding feedback summary
#     doc.add_paragraph(feedback_summary)
# 
#     # Adding questions and feedback status
#     for idx, (question, response) in enumerate(zip(questions, responses)):
#         doc.add_heading(f"Question {idx + 1}", level=1)
#         doc.add_paragraph(f"Question: {question}")
#         doc.add_paragraph(f"Feedback: {response}")
# 
#     buffer = BytesIO()
#     doc.save(buffer)
#     buffer.seek(0)
# 
#     st.download_button(
#         label=label,
#         data=buffer,
#         file_name=filename,
#         mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
#     )
# 
# # Function to extract text from DOCX
# def extract_text_from_docx(docx_file):
#     doc = Document(docx_file)
#     full_text = []
#     for para in doc.paragraphs:
#         full_text.append(para.text)
#     return full_text
# 
# # Function to extract text from PDF (Using PyMuPDF)
# def extract_text_from_pdf(pdf_file):
#     doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     return text.splitlines()
# 
# # Function to load users from CSV
# def load_users_from_csv(file_path):
#     df = pd.read_csv(file_path)
#     return df
# 
# # Function to verify login credentials
# def verify_login(username, password, user_data):
#     if username in user_data['username'].values:
#         stored_password = user_data.loc[user_data['username'] == username, 'password'].values[0]
#         if password == stored_password:
#             return user_data.loc[user_data['username'] == username, 'role'].values[0]
#     return None
# 
# # Function for forgot password screen
# def forgot_password_screen(user_data, csv_file="passwords.csv"):
#     st.subheader("Forgot Password")
# 
#     # Enter username
#     username = st.text_input("Enter your username")
# 
#     # Check if username exists
#     if username and username in user_data['username'].values:
#         st.success("Username found. Please enter your new password.")
# 
#         # Enter new password and confirm it
#         new_password = st.text_input("Enter new password", type="password")
#         confirm_password = st.text_input("Confirm new password", type="password")
# 
#         # Button to submit the new password
#         if st.button("Submit"):
#             if new_password == confirm_password:
#                 # Update the password in the user data and CSV
#                 user_data.loc[user_data['username'] == username, 'password'] = new_password
#                 user_data.to_csv(csv_file, index=False)
# 
#                 # Display success message
#                 st.success("Password updated successfully! Please log in with your new password.")
# 
#                 # Add a 1-second wait before navigating
#                 time.sleep(1)
# 
#                 # Set page to 'login' so that next interaction redirects user to login page
#                 st.session_state['page'] = 'login'
#             else:
#                 st.error("Passwords do not match. Try again.")
#     elif username and username not in user_data['username'].values:
#         st.error("Username not found. Please try again.")
# 
# # Function to extract questions and answers based on delimiters like 'Question' and 'Answer'
# def extract_questions_answers(text):
#     pattern = r"(Question \d+:.*?)(Answer \d+:.*?)($|(?=Question \d+:))"
#     matches = re.findall(pattern, text, re.DOTALL)
# 
#     questions = []
#     responses = []
#     for match in matches:
#         question = match[0].strip()  # Strip whitespace
#         answer = match[1].strip()    # Strip whitespace
#         questions.append(question)
#         responses.append(answer)
# 
#     return questions, responses
# 
# # Function to calculate similarity score between two text pieces
# def calculate_similarity(teacher_response, generated_response):
#     teacher_embedding = embed_model.encode(teacher_response)
#     generated_embedding = embed_model.encode(generated_response)
#     similarity_score = util.cos_sim(teacher_embedding, generated_embedding)
#     return similarity_score.item()
# 
# # Function to generate feedback based on similarity
# def generate_feedback(questions, teacher_responses, generated_responses, threshold=0.75):
#     feedback = []
#     feedback_summary = f"Threshold for similarity: {threshold}\n"
#     feedback_summary += "Conditions:\n"
#     feedback_summary += "If similarity > threshold: Correct\n"
#     feedback_summary += "If similarity == threshold: Correct\n"
#     feedback_summary += "If similarity < threshold: Can be improved\n"
#     feedback_summary += "If similarity == 0: Incorrect\n\n"
# 
#     for idx, (question, teacher_resp, gen_resp) in enumerate(zip(questions, teacher_responses, generated_responses)):
#         similarity = calculate_similarity(teacher_resp, gen_resp)
# 
#         if similarity > threshold or similarity == threshold:
#             status = "Correct"
#         elif similarity < threshold and similarity > 0:
#             status = "Can be improved"
#         else:
#             status = "Incorrect"
# 
#         feedback.append({
#             "Question": question,
#             "Teacher Response": teacher_resp,
#             "Generated Response": gen_resp,
#             "Similarity Score": similarity,
#             "Status": status
#         })
# 
#     return feedback, feedback_summary
# 
# import random
# 
# def generate_questions_from_chapter(chapter, num_questions):
#     # Check if the selected chapter exists in pdf_files
#     if chapter in pdf_files:
#         with open(pdf_files[chapter], "rb") as f:
#             pdf_data = f.read()
#             chapter_text = extract_text_from_pdf(BytesIO(pdf_data))
#             chapter_text = " ".join(chapter_text[:1000])  # Limit excerpt to focus model
# 
#         # Generate questions based on the chapter content
#         questions = []
#         for _ in range(num_questions):
#             # Retry up to 3 times if a question is not generated
#             question = None
#             retries = 3
#             for attempt in range(retries):
#                 question_prompt = (
#                     f"Based on the content in {chapter} only, generate a detailed and relevant question that aligns strictly with the specific SQL topics covered in this chapter. "
#                     f"The question should test understanding of key concepts, techniques, or challenges described in the selected chapter without introducing unrelated material.\n\n"
#                     f"Here is an excerpt from the chapter to focus on:\n\n{chapter_text}\n\n"
#                     "Generate a question that can only be answered using the information covered in this chapter. Avoid general questions about SQL that aren't directly linked to the chapter’s content. "
#                     "The question should encourage learners to think about, apply, or analyze the specific SQL techniques or concepts discussed here. "
#                     "Please make sure that the question remains entirely relevant to this chapter’s topics and does not rely on external knowledge."
#                 )
# 
# 
#                 try:
#                     # Try generating a response
#                     question = generate_response(question_prompt, chapter_text, max_tokens=50)
# 
#                     # Check for valid response
#                     if question and not any(word in question for word in ["nan", "inf"]):
#                         questions.append(question)
#                         break  # Exit retry loop if a valid question is generated
# 
#                 except RuntimeError as e:
#                     # Log the error (optional) or print it for debugging
#                     print(f"Error during question generation: {e}")
# 
#                     # Wait a short time before retrying to avoid immediate retry issues
#                     time.sleep(0.5)
# 
#                 # If retries are exhausted and no question generated, add placeholder
#                 if attempt == retries - 1:
#                     questions.append("No question generated after multiple attempts.")
# 
#         # Display the generated questions
#         st.write("Generated Questions:")
#         for idx, question in enumerate(questions, start=1):
#             st.write(f"{idx}. {question}")
# 
#     else:
#         st.error("Selected chapter not found. Please select a valid chapter.")
# 
# 
# 
# # CSS to style the icon next to the title
# st.markdown(
#     """
#     <style>
#     .title-container {
#         display: flex;
#         align-items: center;
#     }
#     .title-container h1 {
#         margin-left: 10px;
#     }
#     .title-container img {
#         height: 50px;
#         width: 50px;
#     }
#     </style>
#     """, unsafe_allow_html=True
# )
# 
# # Display the title with the icon
# st.markdown(
#     """
#     <div class="title-container">
#         <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSLrK5w3FCWCEYmFRGRaDnG77nwmNBfEUE3Vg&s" alt="SQL Icon">
#         <h1>SQL Query Helper</h1>
#     </div>
#     """, unsafe_allow_html=True
# )
# 
# # Initialize session state for page navigation and login status
# if 'page' not in st.session_state:
#     st.session_state['page'] = 'login'
# if 'logged_in' not in st.session_state:
#     st.session_state.logged_in = False
# if 'role' not in st.session_state:
#     st.session_state.role = None
# 
# # Load users' data from CSV
# users_file = "passwords.csv"  # Ensure this CSV exists with 'username', 'password', and 'role' columns
# user_data = load_users_from_csv(users_file)
# 
# # Sidebar for Logout Button
# if st.session_state.logged_in:
#     if st.sidebar.button("Log Out"):
#         st.session_state.logged_in = False
#         st.session_state.role = None
#         st.session_state.page = 'login'
#         st.sidebar.success("You have been logged out.")
# 
# # Display the appropriate page based on session state
# if st.session_state['page'] == 'login':
#     st.sidebar.header("Login")
# 
#     # Username and Password fields
#     username = st.sidebar.text_input("Username")
#     password = st.sidebar.text_input("Password", type="password")
# 
#     # Login button
#     if st.sidebar.button("Login"):
#         role = verify_login(username, password, user_data)
#         if role:
#             st.session_state.logged_in = True
#             st.session_state.role = role
#             st.sidebar.success(f"Welcome, {username}!")
#         else:
#             st.sidebar.error("Incorrect username or password. Please try again.")
# 
#     # Forgot Password button
#     if st.sidebar.button("Forgot Password"):
#         st.session_state['page'] = 'forgot_password'
# 
# elif st.session_state['page'] == 'forgot_password':
#     forgot_password_screen(user_data, csv_file=users_file)
# 
# # Content after login based on role
# if st.session_state.logged_in and st.session_state.role:
#     st.sidebar.success(f"Logged in as {st.session_state.role}")
# 
#     if st.session_state.role == "student":
#         tab1, tab2 = st.tabs(["Ask a Question", "Generate Responses"])
# 
#         # Tab 1: Ask questions directly
#         with tab1:
#             st.subheader("Ask an SQL-related question:")
#             user_query = st.text_input("Enter your SQL-related question:")
# 
#             if st.button("Submit", key="submit_single"):
#                 if not user_query:
#                     st.write("Please enter a valid SQL-related question.")
#                 else:
#                     relevant_chunk, similarity_score = retrieve_relevant_chunk(user_query, embed_model, chunk_embeddings, document_chunks)
# 
#                     if relevant_chunk is None:
#                         st.write("Sorry, I can only help you with SQL queries.")
#                     else:
#                         generated_response = generate_response(user_query, relevant_chunk, max_tokens=100)
#                         st.write(f"Generated response:\n\n{generated_response}")
# 
#         # Tab 2: Upload and handle CSV, DOCX, PDF files for batch question processing
#         with tab2:
#             st.subheader("Upload a question file (CSV, DOCX, PDF)")
#             uploaded_file = st.file_uploader("Upload a file", type=["csv", "docx", "pdf"])
# 
#             if uploaded_file is not None:
#                 file_type = uploaded_file.type
#                 questions = []
# 
#                 if file_type == "text/csv":
#                     df = pd.read_csv(uploaded_file)
#                     if 'question' in df.columns:
#                         questions = df['question'].tolist()
#                     else:
#                         st.write("Please upload a CSV file with a 'question' column.")
# 
#                 elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
#                     questions = extract_text_from_docx(uploaded_file)
#                     st.write("DOCX File Content Loaded Successfully.")
# 
#                 elif file_type == "application/pdf":
#                     questions = extract_text_from_pdf(uploaded_file)
#                     st.write("PDF File Content Loaded Successfully.")
# 
#                 if questions:
#                     responses = []
# 
#                     for question in questions:
#                         relevant_chunk, similarity_score = retrieve_relevant_chunk(question, embed_model, chunk_embeddings, document_chunks)
# 
#                         if relevant_chunk is None:
#                             response = "Sorry, I can only help you with SQL-related queries."
#                         else:
#                             response = generate_response(question, relevant_chunk, max_tokens=300)
# 
#                         responses.append(response)
# 
#                     st.write("Questions and Generated Responses")
#                     for q, r in zip(questions, responses):
#                         st.write(f"Question: {q}")
#                         st.write(f"Response: {r}")
# 
#                     generate_download_link_as_word(questions, responses, "Generated responses", "generated_responses.docx", "Download Generated Responses")
# 
#     elif st.session_state.role == "teacher":
#         tab1, tab2, tab3, tab4= st.tabs(["Ask a Question", "Generate Responses", "Check Responses", "Generate Questions"])
# 
#         # Tab 1 for Teacher's questions
#         with tab1:
#             st.subheader("Ask an SQL-related question:")
#             user_query = st.text_input("Enter your SQL-related question:")
# 
#             if st.button("Submit", key="submit_single_teacher"):
#                 if not user_query:
#                     st.write("Please enter a valid SQL-related question.")
#                 else:
#                     relevant_chunk, similarity_score = retrieve_relevant_chunk(user_query, embed_model, chunk_embeddings, document_chunks)
# 
#                     if relevant_chunk is None:
#                         st.write("Sorry, I can only help you with SQL queries.")
#                     else:
#                         generated_response = generate_response(user_query, relevant_chunk, max_tokens=100)
#                         st.write(f"Generated response:\n\n{generated_response}")
# 
#         # Tab 2 for Teacher's file upload
#         with tab2:
#             st.subheader("Upload a question file(CSV, DOCX, PDF)")
#             uploaded_file = st.file_uploader("Upload a file", type=["csv", "docx", "pdf"])
# 
#             if uploaded_file is not None:
#                 file_type = uploaded_file.type
#                 questions = []
# 
#                 if file_type == "text/csv":
#                     df = pd.read_csv(uploaded_file)
#                     if 'question' in df.columns:
#                         questions = df['question'].tolist()
#                     else:
#                         st.write("Please upload a CSV file with a 'question' column.")
# 
#                 elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
#                     questions = extract_text_from_docx(uploaded_file)
#                     st.write("DOCX File Content Loaded Successfully.")
# 
#                 elif file_type == "application/pdf":
#                     questions = extract_text_from_pdf(uploaded_file)
#                     st.write("PDF File Content Loaded Successfully.")
# 
#                 if questions:
#                     responses = []
# 
#                     for question in questions:
#                         relevant_chunk, similarity_score = retrieve_relevant_chunk(question, embed_model, chunk_embeddings, document_chunks)
# 
#                         if relevant_chunk is None:
#                             response = "Sorry, I can only help you with SQL-related queries."
#                         else:
#                             response = generate_response(question, relevant_chunk, max_tokens=300)
# 
#                         responses.append(response)
# 
#                     st.write("Questions and Generated Responses")
#                     for q, r in zip(questions, responses):
#                         st.write(f"Question: {q}")
#                         st.write(f"Response: {r}")
# 
#                     generate_download_link_as_word(questions, responses, "Generated responses", "generated_responses.docx", "Download Generated Responses")
# 
#         # Tab 3 for checking responses
#         with tab3:
#             st.subheader("Check Responses")
# 
#             uploaded_response_file = st.file_uploader("Upload a file containing questions and responses (CSV, DOCX, PDF)")
# 
#             if uploaded_response_file is not None:
#                 file_type = uploaded_response_file.type
#                 questions, teacher_responses = [], []
# 
#                 # Extracting from CSV
#                 if file_type == "text/csv":
#                     df = pd.read_csv(uploaded_response_file)
#                     if 'question' in df.columns and 'response' in df.columns:
#                         questions = df['question'].tolist()
#                         teacher_responses = df['response'].tolist()
#                     else:
#                         st.write("Please upload a CSV file with 'question' and 'response' columns.")
# 
#                 # Extracting from DOCX or PDF
#                 elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document" or file_type == "application/pdf":
#                     if file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
#                         doc_text = extract_text_from_docx(uploaded_response_file)
#                     elif file_type == "application/pdf":
#                         doc_text = extract_text_from_pdf(uploaded_response_file)
# 
#                     # Use the regex-based function to extract questions and responses
#                     questions, teacher_responses = extract_questions_answers(" ".join(doc_text))
# 
#                 if questions and teacher_responses:
#                     # Generate responses using the LLM
#                     generated_responses = []
#                     for question in questions:
#                         relevant_chunk, _ = retrieve_relevant_chunk(question, embed_model, chunk_embeddings, document_chunks)
#                         generated_responses.append(generate_response(question, relevant_chunk, max_tokens=300))
# 
#                     # Compare the teacher's responses with the LLM-generated responses and calculate feedback
#                     feedback, feedback_summary = generate_feedback(questions, teacher_responses, generated_responses)
# 
#                     # Display feedback
#                     st.write("Feedback on Responses")
#                     for item in feedback:
#                         st.write(f"**Question**: {item['Question']}")
#                         st.write(f"**Teacher's Response**: {item['Teacher Response']}")
#                         st.write(f"**Similarity Score**: {item['Similarity Score']:.2f}")
#                         st.write(f"**Status**: {item['Status']}")
#                         st.write("---")
# 
#                     # Convert feedback to DataFrame for download
#                     feedback_df = pd.DataFrame(feedback)
#                     generate_download_link_as_word(
#                         feedback_df['Question'],
#                         feedback_df['Status'],
#                         feedback_summary,
#                         "feedback_responses.docx",
#                         "Download Feedback Generated"
#                     )
# 
#         with tab4:
#             st.subheader("Generate Open-ended Questions from Selected Chapter")
# 
#             # Select chapter from 1 to 7
#             chapter = st.selectbox("Select Chapter", [f"Chapter {i}" for i in range(1, 8)])
#             num_questions = st.number_input("Number of questions to generate", min_value=1, max_value=5, step=1)
# 
#             if st.button("Generate Questions"):
#                 generate_questions_from_chapter(chapter, num_questions)
# 
# 
# # Call the function to show the PDF download links in the sidebar
# generate_download_button_for_pdfs()
#

!streamlit run app.py &>/content/logs.txt &

!kill $(ps aux | grep 'ngrok' | awk '{print $2}')

from pyngrok import ngrok

public_url = ngrok.connect(8501)
print(f"Public URL: {public_url}")

