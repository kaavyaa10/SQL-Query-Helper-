# -*- coding: utf-8 -*-
"""load_llm_model.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yI45JZqF_GvJU9LRkNd0hZpcZzzGfwVc
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers vllm langchain

!pip install sentence-transformers

# Uninstall current versions of torch, torchvision, and torchaudio
!pip uninstall torch torchvision torchaudio -y

# Install the correct versions of torch and torchvision required by VLLM
!pip install torch==2.4.0 torchvision==0.19.0 torchaudio --index-url https://download.pytorch.org/whl/cu121

!huggingface-cli login

from vllm import LLM
from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("TheBloke/Mistral-7B-v0.1-AWQ")

# Load the model
model_name = "TheBloke/Mistral-7B-v0.1-AWQ"
llm = LLM(model=model_name)

# Define your input query
query = "What is SQL?"

# Tokenize the input query
inputs = tokenizer(query, return_tensors="pt")

# Generate a response using vLLM
outputs = llm.generate(query)

# Extract the generated text from the outputs
response_text = outputs[0].outputs[0].text

# Print the generated response
print("Mistral Model Response: ", response_text)

from google.colab import files
uploaded = files.upload()

import zipfile
import os
import pandas as pd

# Path to the uploaded ZIP file
zip_file_path = 'Dataset.zip'
extract_path = '/content/extracted_dataset/'

# Extract the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List the extracted files
extracted_files = os.listdir(extract_path)
print("Extracted files:", extracted_files)

# Load the CSV files and display the first few rows of each
csv_files = [f for f in extracted_files if f.endswith('.csv')]

for csv_file in csv_files:
    file_path = os.path.join(extract_path, csv_file)
    df = pd.read_csv(file_path)
    print(f"\nPreview of {csv_file}:")
    print(df.head())

dataset_folder_path = os.path.join(extract_path, 'Dataset')

# List the files
dataset_files = os.listdir(dataset_folder_path)
print("Files inside 'Dataset' folder:", dataset_files)

# Load  and display the first few rows
csv_files = [f for f in dataset_files if f.endswith('.csv')]

for csv_file in csv_files:
    file_path = os.path.join(dataset_folder_path, csv_file)
    df = pd.read_csv(file_path)
    print(f"\nPreview of {csv_file}:")
    print(df.head())

from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
import os

# Path to the folder containing the CSV files
dataset_folder_path = '/content/extracted_dataset/Dataset'

# Initialize the Sentence Transformer model for embedding
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# Loop through all the CSV files and generate embeddings
all_embeddings = []
csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]

for csv_file in csv_files:
    file_path = os.path.join(dataset_folder_path, csv_file)

    # Load the CSV file
    df = pd.read_csv(file_path)

    # Extract the text content from the 'Extracted_Text' column
    documents = df['Extracted_Text'].tolist()

    # Generate embeddings for the documents in this CSV
    embeddings = embed_model.encode(documents)

    # Append embeddings to the list
    all_embeddings.append(embeddings)

# Convert the list of embeddings into a single numpy array
all_embeddings = np.vstack(all_embeddings)

# Save the embeddings to a file for future use
np.save('chapter_document_embeddings.npy', all_embeddings)

print("Embeddings for all chapters created and saved successfully!")

# Specify the path where you want to save the embeddings in your Google Drive
drive_save_path = '/content/drive/MyDrive/chapter_document_embeddings.npy'

# Save the embeddings to your Google Drive
np.save(drive_save_path, all_embeddings)

print(f"Embeddings saved to: {drive_save_path}")

# Save the tokenizer to Google Drive
tokenizer_save_path = '/content/drive/MyDrive/mistral_tokenizer'
tokenizer.save_pretrained(tokenizer_save_path)

print(f"Tokenizer saved to: {tokenizer_save_path}")

import shutil
import os

# Path to the folder containing your extracted CSV files
dataset_folder_path = '/content/extracted_dataset/Dataset'

# Path where you'd like to save your dataset on Google Drive
drive_dataset_folder_path = '/content/drive/MyDrive/saved_dataset'

# Check if the target folder already exists
if os.path.exists(drive_dataset_folder_path):
    # Remove the existing folder and its contents
    shutil.rmtree(drive_dataset_folder_path)

# Copy the entire folder to Google Drive
shutil.copytree(dataset_folder_path, drive_dataset_folder_path)

print(f"Dataset saved to: {drive_dataset_folder_path}")

!ls /content/drive/MyDrive/TheBloke_Mistral-7B-v0.1-AWQ

from transformers import AutoTokenizer, AutoModelForCausalLM

# Download tokenizer and model from Hugging Face
model_name = "TheBloke/Mistral-7B-v0.1-AWQ"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')

# Re-save the model and tokenizer to Google Drive for future use
model_save_path = '/content/drive/MyDrive/TheBloke_Mistral-7B-v0.1-AWQ'
tokenizer.save_pretrained(model_save_path)
model.save_pretrained(model_save_path)

print("Model and tokenizer re-saved successfully!")

!ls /content/drive/MyDrive/TheBloke_Mistral-7B-v0.1-AWQ

