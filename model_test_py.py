# -*- coding: utf-8 -*-
"""Model_Test.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rpiZXhwqKzTaiDDstea3QRNAgz5LU9iE
"""

from google.colab import drive
drive.mount('/content/drive')

!cat /content/drive/MyDrive/Colab\ Notebooks/Model_Test.py

!pip install transformers

!pip install torch==2.3.1+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

!pip install sentence-transformers

!pip install autoawq

import torch
print(torch.__version__)

import torch
print(torch.cuda.is_available())

from transformers import AutoTokenizer, AutoModelForCausalLM

# Define the path where the model is saved
model_save_path = '/content/drive/MyDrive/TheBloke_Mistral-7B-v0.1-AWQ'

# Load the tokenizer and model using the slow tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_save_path, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_save_path).to('cuda')

print("Model and tokenizer loaded successfully!")

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import os

# Step 1: Paths to the saved files in Google Drive

# Path to dataset embeddings and document chunks
embeddings_path = '/content/drive/MyDrive/chapter_document_embeddings.npy'
dataset_folder_path = '/content/drive/MyDrive/saved_dataset'
model_name = 'all-MiniLM-L6-v2'  # Same model used for embeddings

# Path to document chunks and their embeddings
chunks_path = '/content/drive/MyDrive/document_chunks.npy'
chunk_embeddings_path = '/content/drive/MyDrive/chunk_embeddings.npy'

# Step 2: Load all components

# Load document embeddings
print("Loading document embeddings...")
document_embeddings = np.load(embeddings_path)

# Load the Sentence Transformer model (for embedding user queries)
print("Loading Sentence Transformer model...")
embed_model = SentenceTransformer(model_name)

# Load the CSV files (to retrieve the original document text)
print("Loading CSV files from dataset...")
csv_files = [f for f in os.listdir(dataset_folder_path) if f.endswith('.csv')]
documents = []
for csv_file in csv_files:
    file_path = os.path.join(dataset_folder_path, csv_file)
    df = pd.read_csv(file_path)
    documents.extend(df['Extracted_Text'].tolist())

# Load document chunks and chunk embeddings
print("Loading document chunks and embeddings...")
document_chunks = np.load(chunks_path, allow_pickle=True)
chunk_embeddings = np.load(chunk_embeddings_path)

print("All components loaded successfully!")

!ls /content/drive/MyDrive/TheBloke_Mistral-7B-v0.1-AWQ

from sentence_transformers import util

# Function to retrieve the most relevant chunk based on user query
def retrieve_relevant_chunk(query, embed_model, chunk_embeddings, document_chunks, threshold=0.3):
    query_embedding = embed_model.encode(query)
    similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]
    best_match_idx = np.argmax(similarities)

    # Get the highest similarity score
    best_similarity = similarities[best_match_idx].item()

    # If the similarity score is below the threshold, return None (no relevant context)
    if best_similarity < threshold:
        return None, best_similarity

    best_chunk = document_chunks[best_match_idx]
    return best_chunk, best_similarity

# Function to generate a response using the Mistral model, focusing on using only the dataset's context
def generate_response(query, context, max_tokens=100):
    # Revised prompt based on user instruction
    prompt = (
        f"You are a helpful assistant. You will use the provided context to answer user questions. "
        f"Read the given context before answering the question and think step by step. "
        f"If you cannot answer the user's question based on the provided context, inform the user. "
        f"Do not use any other information for answering the question. "
        f"Provide a detailed answer for the question if you find the context.\n\n"
        f"Question: {query}\n\n"
        f"Answer:"
    )

    # Tokenize the prompt
    inputs = tokenizer(prompt, return_tensors="pt").to('cuda')

    # Generate a response with limits on the number of tokens and repetition penalty
    outputs = model.generate(
        inputs['input_ids'],
        max_new_tokens=max_tokens,
        do_sample=True,
        top_p=0.9,  # Reduce the sampling diversity to focus more on accuracy
        temperature=0.7,  # Keep randomness to some extent to generate natural responses
        pad_token_id=tokenizer.eos_token_id,  # Ensure appropriate ending
        repetition_penalty=1.3  # Penalize repetitions to avoid repeated text
    )

    # Decode the generated response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Return only the generated answer and strip any remaining parts of the prompt
    answer_start = response.find("Answer:") + len("Answer:")
    return response[answer_start:].strip()

# Main interaction loop for user queries
if __name__ == "__main__":
    while True:
        # Get a query from the user
        user_query = input("Enter your question (or type 'exit' to quit): ")

        # Exit the loop if the user types 'exit'
        if user_query.lower() == 'exit':
            print("Exiting the system. Goodbye!")
            break

        # Retrieve the most relevant chunk from the dataset, along with the similarity score
        relevant_chunk, similarity_score = retrieve_relevant_chunk(user_query, embed_model, chunk_embeddings, document_chunks)

        # Check if the similarity score is below the threshold (no relevant context)
        if relevant_chunk is None:
            print("Sorry, I can only help you with SQL queries.")
            continue

        # Display the similarity score for debugging (optional)
        print(f"Similarity score: {similarity_score:.2f}")

        # Generate a response using the retrieved chunk as context
        generated_response = generate_response(user_query, relevant_chunk, max_tokens=100)

        # Display only the generated response, not the context
        print(f"\nGenerated response:\n{generated_response}\n")

"""**EVALUATION MATRIX**

1.   Bleu Score (Bilingual Evalauation Understudy) is used to evaluate the quality of machine generated text by comparing it to document
"""

pip install nltk

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
nltk.download('punkt')

# Function to calculate BLEU score
def calculate_bleu(reference, hypothesis):
    reference_tokens = [nltk.word_tokenize(reference)]
    hypothesis_tokens = nltk.word_tokenize(hypothesis)

    smoothing_function = SmoothingFunction().method4

    # Calculate BLEU score
    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoothing_function)

    return bleu_score

questions = [
    "What is SQL?",
    "Explain the syntax of TRUNCATE TABLE.",
    "What is a primary key?",
    "Explain the concept of foreign key with an example.",
    "How do you create a database?"
]

reference_answers = [
    "SQL is a language used to manage and query relational databases.",
    "The syntax for TRUNCATE TABLE is: TRUNCATE TABLE table_name.",
    "A primary key uniquely identifies each row in a table.",
    "A foreign key links two tables by referencing the primary key in one table from another table. For example, a customer ID in an orders table can be a foreign key referencing the ID column in a customers table.",
    "To create a database in SQL, you can use the CREATE DATABASE statement."
]

def generate_model_answer(query):
    prompt = f"Based on the following context, provide a concise and accurate answer using only the information provided in the context, including relevant examples or code snippets:\n\nQuestion: {query}\n\nAnswer:"

    inputs = tokenizer(prompt, return_tensors="pt").to('cuda')

    outputs = model.generate(
        inputs['input_ids'],
        max_new_tokens=100,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
        repetition_penalty=1.3
    )


    response = tokenizer.decode(outputs[0], skip_special_tokens=True)


    answer_start = response.find("Answer:") + len("Answer:")
    return response[answer_start:].strip()

# Evaluate BLEU scores for multiple questions
total_bleu_score = 0
num_questions = len(questions)

for i in range(num_questions):
    question = questions[i]
    reference = reference_answers[i]

    generated_answer = generate_model_answer(question)

    bleu_score = calculate_bleu(reference, generated_answer)
    print(f"Question: {question}")
    print(f"Reference Answer: {reference}")
    print(f"Generated Answer: {generated_answer}")
    print(f"BLEU Score: {bleu_score:.4f}\n")

    total_bleu_score += bleu_score

average_bleu_score = total_bleu_score / num_questions
print(f"Average BLEU Score: {average_bleu_score:.4f}")

"""2. Rouge score measures the overlap of
n-grams (sequences of 1, 2, or more words) between the generated text and the reference text.Rouge Score 1: overalp of single words, Rouge Score 2: overlap of two word seuence, Rouge Score L: Longest common subsequence between texts




"""

pip install rouge-score

from rouge_score import rouge_scorer

# Function to calculate ROUGE score
def calculate_rouge(reference, hypothesis):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, hypothesis)

    return scores

total_rouge1, total_rouge2, total_rougeL = 0, 0, 0

for i in range(num_questions):
    reference = reference_answers[i]
    generated_answer = generate_model_answer(questions[i])

    # Calculate ROUGE score
    rouge_scores = calculate_rouge(reference, generated_answer)

    print(f"Question: {questions[i]}")
    print(f"Reference Answer: {reference}")
    print(f"Generated Answer: {generated_answer}")
    print(f"ROUGE-1: {rouge_scores['rouge1'].fmeasure:.4f}")
    print(f"ROUGE-2: {rouge_scores['rouge2'].fmeasure:.4f}")
    print(f"ROUGE-L: {rouge_scores['rougeL'].fmeasure:.4f}\n")

    total_rouge1 += rouge_scores['rouge1'].fmeasure
    total_rouge2 += rouge_scores['rouge2'].fmeasure
    total_rougeL += rouge_scores['rougeL'].fmeasure


average_rouge1 = total_rouge1 / num_questions
average_rouge2 = total_rouge2 / num_questions
average_rougeL = total_rougeL / num_questions

print(f"Average ROUGE-1: {average_rouge1:.4f}")
print(f"Average ROUGE-2: {average_rouge2:.4f}")
print(f"Average ROUGE-L: {average_rougeL:.4f}")

"""3. Perplexity, which is used to tell how well a model predicts text"""

import torch

def calculate_perplexity(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="pt").to('cuda')
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs['input_ids'])
        loss = outputs.loss
        perplexity = torch.exp(loss)
    return perplexity.item()

for i in range(num_questions):
    generated_answer = generate_model_answer(questions[i])

    perplexity = calculate_perplexity(generated_answer, model, tokenizer)

    print(f"Question: {questions[i]}")
    print(f"Generated Answer: {generated_answer}")
    print(f"Perplexity: {perplexity:.4f}\n")

"""4. Gleu Score, similar to bleu score but consider both precision and recall."""

from nltk.translate.gleu_score import sentence_gleu

def calculate_gleu(reference, hypothesis):
    reference_tokens = [nltk.word_tokenize(reference)]
    hypothesis_tokens = nltk.word_tokenize(hypothesis)

    gleu_score = sentence_gleu(reference_tokens, hypothesis_tokens)
    return gleu_score

total_gleu_score = 0

for i in range(num_questions):
    reference = reference_answers[i]
    generated_answer = generate_model_answer(questions[i])

    # Calculate GLEU score
    gleu_score = calculate_gleu(reference, generated_answer)

    print(f"Question: {questions[i]}")
    print(f"Reference Answer: {reference}")
    print(f"Generated Answer: {generated_answer}")
    print(f"GLEU Score: {gleu_score:.4f}\n")

    total_gleu_score += gleu_score

average_gleu_score = total_gleu_score / num_questions
print(f"Average GLEU Score: {average_gleu_score:.4f}")

"""5. Semantic Textual Similarity(STS) measuers how similar texts are in meaning, and not just comparing texts

"""

from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer('all-MiniLM-L6-v2')

def calculate_sts(reference, hypothesis):
    reference_embedding = embed_model.encode([reference])
    hypothesis_embedding = embed_model.encode([hypothesis])

    similarity = cosine_similarity(reference_embedding, hypothesis_embedding)

    return similarity[0][0]

total_sts_score = 0

for i in range(num_questions):
    reference = reference_answers[i]
    generated_answer = generate_model_answer(questions[i])

    # Calculate STS score
    sts_score = calculate_sts(reference, generated_answer)

    print(f"Question: {questions[i]}")
    print(f"Reference Answer: {reference}")
    print(f"Generated Answer: {generated_answer}")
    print(f"STS (Cosine Similarity): {sts_score:.4f}\n")

    total_sts_score += sts_score

average_sts_score = total_sts_score / num_questions
print(f"Average STS (Cosine Similarity) Score: {average_sts_score:.4f}")

"""FINE TUNE

"""

from sentence_transformers import util

def retrieve_relevant_chunk(query, embed_model, chunk_embeddings, document_chunks):
    query_embedding = embed_model.encode(query)
    similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]
    best_match_idx = np.argmax(similarities)
    best_chunk = document_chunks[best_match_idx]
    return best_chunk

def generate_response(query, context, max_tokens=200):
    prompt = f"{query}:\n\nProvide the answer directly without adding extra questions or information."

    inputs = tokenizer(prompt, return_tensors="pt").to('cuda')
    outputs = model.generate(
        inputs['input_ids'],
        max_new_tokens=max_tokens,
        num_beams=5,
        early_stopping=True,
        pad_token_id=tokenizer.eos_token_id
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)


    return response.strip()

if __name__ == "__main__":
    while True:
        user_query = input("Enter your question (or type 'exit' to quit): ")

        if user_query.lower() == 'exit':
            print("Exiting the system. Goodbye!")
            break


        relevant_chunk = retrieve_relevant_chunk(user_query, embed_model, chunk_embeddings, document_chunks)
        generated_response = generate_response(user_query, relevant_chunk, max_tokens=200)

        print(f"\nGenerated response:\n{generated_response}\n")

from sentence_transformers import util

def retrieve_relevant_chunk(query, embed_model, chunk_embeddings, document_chunks):
    query_embedding = embed_model.encode(query)
    similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]
    best_match_idx = np.argmax(similarities)
    best_chunk = document_chunks[best_match_idx]
    return best_chunk

def generate_response(query, context, max_tokens=200):
    prompt = f"{query}:"

    inputs = tokenizer(prompt, return_tensors="pt").to('cuda')
    outputs = model.generate(
        inputs['input_ids'],
        max_new_tokens=max_tokens,
        num_beams=5,
        early_stopping=True,
        pad_token_id=tokenizer.eos_token_id
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response.strip()

if __name__ == "__main__":
    while True:
        user_query = input("Enter your question (or type 'exit' to quit): ")

        if user_query.lower() == 'exit':
            print("Exiting the system. Goodbye!")
            break

        relevant_chunk = retrieve_relevant_chunk(user_query, embed_model, chunk_embeddings, document_chunks)

        generated_response = generate_response(user_query, relevant_chunk, max_tokens=200)

        print(f"\nGenerated response:\n{generated_response}\n")